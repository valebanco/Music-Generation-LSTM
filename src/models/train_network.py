# -*- coding: utf-8 -*-
"""network_csv_representation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dlADd-U6FoTB8Q1DjAbi_lgZ_vYCojh1
"""

""" This module prepares midi file data and feeds it to the neural
	network for training """

import matplotlib.pyplot as plt
import pickle
import tensorflow as tf
import numpy as np
from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay
from keras.models import Sequential,load_model
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.layers import GRU
from keras.layers import Activation
from keras.layers import BatchNormalization as BatchNorm
from keras.layers import concatenate
from keras.layers import Input
from keras.layers import Bidirectional

from keras.callbacks import ModelCheckpoint

from keras.optimizers import adam_v2

from keras import Model

#ADDESTRAMENTO CON CORE LSTM STANDARD
def train_network(
  ROOT_PATH_DATA,PATH_MODEL,
  epochs,batch_size,verbose
  ):
    print ("read dataset...")
    X_train = load_set("X_train.pickle",ROOT_PATH_DATA)
    y_train = load_set("y_train.pickle",ROOT_PATH_DATA)
    X_validation = load_set("X_validation.pickle",ROOT_PATH_DATA)
    y_validation = load_set("y_validation.pickle",ROOT_PATH_DATA)
    X_test = load_set("X_test.pickle",ROOT_PATH_DATA)
    y_test = load_set("y_test.pickle",ROOT_PATH_DATA)
    
    print("create network skeleton")
    
    model = create_network(
        X_train[0],y_train[0].shape[1],
        X_train[1], y_train[1].shape[1],
        X_train[2], y_train[2].shape[1],
        X_train[3], y_train[3].shape[1],
        X_train[4], y_train[4].shape[1]
    )

    print("prepare training...")
    
    
    train_with_validation_data(
        model,
        X_train,y_train,
        X_validation,y_validation,
        X_test,y_test,
        PATH_MODEL,epochs,batch_size,verbose
    )
  
    
def network_branch_to_consider(network_input):
  """
  si distinguono due tipologie di input (che sono output di tale funzione):
  inputin --> ESSI COSTITUISCONO IL NODO DI INPUT DELLA RETE NEURALE E NON PER LA COSTRUZIONE DEL MODELLO
  inputLayer -->  input utili alla costruzione del modello Model (DEFINIZIONE DELLO SPAZIO RISERVATO ALL'INPUT)
  """
  inputLayer = Input(shape=(network_input.shape[1], network_input.shape[2]))
  inputin = LSTM(
    256,
    input_shape=(network_input.shape[1], network_input.shape[2]),
    return_sequences=True
  )(inputLayer)
  inputin = Dropout(0.5)(inputin)

  return inputin,inputLayer

def network_branch_to_classify(string_desc,result_RNN,n_vocab):
  """
  il risultato di tale funzione COSTITUISCE IL NODO DI OUTPUT DELLA RETE NEURALE
  """
  output = Dense(128, activation='relu')(result_RNN)
  output = BatchNorm()(output)
  output = Dropout(0.5)(output)
  output = Dense(n_vocab, activation='softmax', name=string_desc)(output)
  return output

def create_network(
    network_input_notes,n_vocab_notes,
    network_input_offsets, n_vocab_offsets,
    network_input_durations, n_vocab_durations,
    network_input_velocities, n_vocab_velocities,
    network_input_tempos, n_vocab_tempos
    ):
  """
  in tale funzione network_input_notes,network_input_offsets,network_input_durations sono usati
  ESCLUSIVAMENTE NON PER IL LORO CONTENUTO MA PER LE LORO DIMENSIONI O SHAPE
  """

  #costruzione dei due differenti input
  inputNotes,inputNotesLayer = network_branch_to_consider(network_input_notes)
  inputOffsets,inputOffsetsLayer = network_branch_to_consider(network_input_offsets)
  inputDurations,inputDurationsLayer = network_branch_to_consider(network_input_durations)
  inputVelocities,inputVelocitiesLayer = network_branch_to_consider(network_input_velocities)
  inputTempos,inputTemposLayer = network_branch_to_consider(network_input_tempos)

  # concatenazione degli input da dare in pasto alla rete neurale GRU
  inputs = concatenate([inputNotes, inputOffsets, inputDurations,inputVelocities,inputTempos])

  # costruzione della GRU proposta nel documento per considerare cosa ha imparato nei tre differenti Branches

  x = LSTM(512, return_sequences=True)(inputs)
  x = Dropout(0.5)(x)
  x = LSTM(512)(x)
  x = BatchNorm()(x)
  x = Dropout(0.5)(x)
  x = Dense(256, activation='relu')(x)

  outputNotes = network_branch_to_classify("Note",x,n_vocab_notes)
  outputOffsets = network_branch_to_classify("Offset",x,n_vocab_offsets)
  outputDurations = network_branch_to_classify("Duration",x,n_vocab_durations)
  outputVelocities = network_branch_to_classify("Velocity",x,n_vocab_velocities)
  outputTempos = network_branch_to_classify("Tempo",x,n_vocab_tempos)

  model = Model(
    inputs=[inputNotesLayer, inputOffsetsLayer, inputDurationsLayer,inputVelocitiesLayer,inputTemposLayer], 
    outputs=[outputNotes, outputOffsets, outputDurations,outputVelocities,outputTempos]
  )

  #compilazione del modello
  model.compile(
    loss='categorical_crossentropy',
    metrics= ['acc'],
    optimizer='adam'
  )
  # Useful to try RMSProp though
  #model.compile(loss='categorical_crossentropy', optimizer= "RMSprop")
  model.summary()
  return model


def train_with_validation_data(
	model, 
	X_train,y_train,
	X_val,y_val,
  X_test,y_test,
	PATH_MODEL,epochs,batch_size,verbose):
	
  """
  in tale funzione di training i dati di input e di output vengono immessi 
  ESCLUSIVAMENTE CONSIDERANDO IL LORO CONTENUTO
  """
  filepath = "weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5"

  checkpoint_loss = ModelCheckpoint(
    filepath,
    period = 10,
    monitor='loss',
    verbose=verbose,
    save_best_only=True,
    mode='min'
  )

  callbacks_list = [checkpoint_loss]
  """
  l'addestramenta avr√†:
  - la serie di input estratte dalla parserizzazione dei file midi
  - la serie di output corrispondenti agli input estratti nella parserizzazione (OUTPUT NON DI PREDIZIONE)
  """
  #addestramento con dati di validation definiti in dati
  history = model.fit(
    X_train, 
    y_train,
    validation_data=(X_val,y_val),
    epochs = epochs, 
    batch_size = batch_size,
    callbacks= callbacks_list
    )
  #evaluate_model(model,batch_size,X_test,y_test)
  plot_accuracy_validation(history)
  plot_loss_validation(history)

  model.save(PATH_MODEL)

def plot_accuracy_validation(history):
  print(history.history.keys())
  complex_acc = [history.history['Note_acc'],history.history['Offset_acc'],history.history['Duration_acc'],history.history['Velocity_acc'],history.history['Tempo_acc']]
  summed_acc = list(map(sum, zip(*complex_acc)))
  
  complex_val_acc = [history.history['val_Note_acc'],history.history['val_Offset_acc'],history.history['val_Duration_acc'],history.history['val_Velocity_acc'],history.history['val_Tempo_acc']]
  summed_val_acc = list(map(sum, zip(*complex_val_acc)))
  
  n_feature = 5
  acc = [x / n_feature for x in summed_acc]
  val_acc = [x / n_feature for x in summed_val_acc]

  save_history_element("accuracy_training.npz",acc)
  save_history_element("val_accuracy_training.npz",val_acc)

  epochs = range(1, len(acc) + 1)
  plt.plot(epochs, acc, 'bo', label='Training accuracy')
  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.show()

def save_history_element(filename,array):
   np.savez_compressed(filename,array)
  
def plot_loss_validation(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']


  save_history_element("loss_training.npz",loss)
  save_history_element("val_loss_training.npz",val_loss)

  epochs = range(1, len(loss) + 1)
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.show()
def load_set (name_set,path):
    filepath1 = open(path + "/" + name_set, 'rb')
    r_set = pickle.load(filepath1)
    return r_set

train_network(
    "../../data/processed/dataset_pickle","../../models/model",
	  15,128,0
    )

